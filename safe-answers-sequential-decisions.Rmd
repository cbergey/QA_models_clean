---
title: "What's a good question?"
subtitle: "Safe answers & sequential decisions"
author: ""
date: "2021-10-05"
output: 
  github_document:
  pandoc_args: --webtex
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(rwebppl)
library(tidyverse)
library(aida) # remotes::install_github("michael-franke/aida-package")
```

```{r echo = F}
run_plot_model <- function (context_name = "pieCakeContext", questionerUtilFct = "KL") {
  webPPL_data = tibble('context' = context_name, 'questionerUtilFct' = questionerUtilFct)
  webppl(
    program_file = "qa-models-sequential-decisions.webppl",
    data = webPPL_data,
    data_var = "myDF"
  ) -> output

  out_plot <- output %>%
    as_tibble() %>%
    mutate(support = fct_reorder(support, prob)) %>%
    ggplot(aes(x = support, y = prob)) +
    geom_col(fill = "#505B55") + coord_flip() +
    xlab("") +
    ylab("") +
    ggtitle(context_name) + 
    theme_aida()

  factor = 3

  # ggsave(filename = str_c("pics/results-", context_name, ".pdf"), width = 16/factor, height = 9/factor)

  out_plot
  
}
```


## Model specification

The base-level respondent is a literal respondent who selects any true, safe answer with equal probability. 
Utterance costs are set to 0 and play no role here.

The base-level respondent can be characterized as an agent who selects utterances based on a maximim criterion for a utility function that gives payoff 1 for informative answers and 0 for uninformative answers, under the belief that the questioner may hold any logically possible belief compatible with uncertainty about the answer to the question asked.

## Minimal base case

Quinn knows that there is only one available item.
Quinn has no preferences and no *a priori* beliefs.
Questions are therefore restricted to item-questions and the most general 'which' question.

```{r, echo = F}
run_plot_model("pieCakeContextMinimal")
```

## Minimal w/ preferences

Quinn knows that there is only one available option.
Questions are therefore restricted to item-questions and the most general 'which' question.
Quinn's preferences are:

- RP = 5
- LC = 3
- SC = 1
- AS = 1

We expect that $P(RP?) > P(LC?) > P(SC?) = P(AS?)$.

```{r echo = F}
run_plot_model("pieCakeContextMinimalWithPreferences")
```


## Richer neutral context

Quinn knows that any $k$ items might be available.
Available questions contain (two-place) disjunctive questions and feature-questions (e.g., "Do you have cake?").
Notice that feature-questions are disjunctive as well.
Quinn has unbiased beliefs, but has preferences for cakes as before:

- RP = 5
- LC = 3
- RC = 1
- LP = 1

We expect the same order among single-polar questions as before: $P(RP?) > P(LC?) > P(SC?) = P(AS?)$.
We also expect that the odds for the *which*-question against "RP?" are decreased in comparison to the previous more minimal case.
We also expect that the disjunctive question "RP or LC?" is the best among all disjunctive and feature-questions.

```{r echo = F}
run_plot_model("pieCakeContext")
```

## Feature-based preferences

If preferences over outcomes are the result of simple additive preferences over features, we have clear intuitions regarding which feature-related question to ask.
In particular, if one feature contributes more to overall utility than another, it is intuitively more natural to ask for the preferred feature.

Assume that Quinn has the following preferences.
Overall, the topping (raspberry vs lemon) contributes more to Quinn's payoffs than the type of baked good (pie vs cake).
Each feature-value contributes the following (additive) score to Quinn's payoff:

- raspberry = 6
- lemon = 4
- pie = 3
- cake = 2

Together this yields the following utils:

- raspberry pie = 9
- raspberry cake = 8
- lemon pie = 7
- lemon cake = 6


```{javascript}
// amended context with feature-based (additive) preferences
var pieCakeContextAdditivePreferences = extend(
  pieCakeContext,
  {
    // feature-additive preferences: pie = 3, cake = 2, rasp = 6, lemon = 4;
    decisionProblem: function(w, a) {
      return _.includes(w, a) ? (
        a == 'raspPie' ? 9 :
          a == 'raspCake' ? 8 :
          a == 'lemonPie' ? 7 :
          a == 'lemonCake' ? 6 : 
          console.error('unknown action')
      ) : 0.0000001;
    }
  }
)
```

Intuitively, we expect the following order on feature-related questions.
First, we expect to see questions for the preferred feature value to be more likely than questions for the non-preferred feature-value:

$$P(\text{raspberry?}) > P(\text{lemon?})$$

$$P(\text{pie?}) > P(\text{cake?})$$

Second, we expect to see (the most probable) questions after the "more relevant" feature to be more likely than those of a less relevant feature:

$$P(\text{raspberry?}) > P(\text{pie?})$$

Here are the predictions for this case:

```{r, echo = F}
run_plot_model("pieCakeContextAdditivePreferences")
```

## Influence of biased prior beliefs

If Quinn has no preferences but strong prior biases that either RP or LC are true, we expect the odds for the 'RP?' question against the general 'which' question to increase in comparison to a case with no prior biases.

Here are the predictions for a no-preferences, no-biases case:

```{r, echo = F}
run_plot_model("pieCakeContextUnbiasedNoPref")
```


Compared to the case of strong prior beliefs that either RP or LC is true:

```{r, echo = F}
run_plot_model("pieCakeContextBiasedNoPref")
```


